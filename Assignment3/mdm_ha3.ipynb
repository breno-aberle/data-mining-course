{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1 has been done on an external software Gephi, and therefore I wrote the report on a word document. The report has been merged to the beginning of this pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (159, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n1  n2\n",
       "0  11   1\n",
       "1  15   1\n",
       "2  16   1\n",
       "3  41   1\n",
       "4  43   1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data set\n",
    "number_nodes = 62\n",
    "\n",
    "df = pd.read_csv('t2_dolphins.txt', sep=\"\\s+\", header=None, names=[\"n1\", \"n2\"]) #\n",
    "print(\"data shape: \", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Questions</b> \n",
    "\n",
    "* In this task only undirected graphs are considered, since the professors confirmed that only undirected graphs need to be analyzed.\n",
    "* For undirected graphs, the notion of density of the subgraph is the average degree of the subgraph.\n",
    "* First thought:\n",
    "    * Search node with highest degree\n",
    "    * of all neighbors search node with highest degree and add it to the subgraphs\n",
    "    * out of all current nodes in the subgraph find neighbor with highest degree and add it to the subgraph\n",
    "* However, the paper 'Greedy Approximation Algortihms for Finding Dense Components in a Graph' by Moses Charikar analyzes another algortihm that is more efficent and promises better results:\n",
    "    * Given a number a set of vertices that build a graph, remove in every step the vertice with lowest degree\n",
    "    * Calculate in each step the density and store for each step the density and vertices of used subgraph\n",
    "    * Do that till all nodes are deleted\n",
    "    * return subgraph with highest density\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_density(subgraph):\n",
    "    \"\"\" Calculate density of subgraph according to formula given in task description\n",
    "    \"\"\"\n",
    "    number_edges = subgraph.shape[0]\n",
    "    number_nodes = len(np.unique(subgraph))\n",
    "    \n",
    "    density = number_edges / number_nodes\n",
    "    \n",
    "    return density\n",
    "\n",
    "def greedy_dense_subgraph(graph):\n",
    "    \"\"\" Greedy algorithm to find a subgraph with good density\n",
    "    \"\"\"\n",
    "    number_nodes = len(np.unique(graph))\n",
    "    current_subgraph = graph\n",
    "    subgraph_density_history = []  # keep track of subraphs and their density score\n",
    "    density_score_history = []\n",
    "    number_edges_history = []\n",
    "    \n",
    "    while (number_nodes > 0):\n",
    "        \n",
    "        # calculate density\n",
    "        density_score = calculate_density(current_subgraph)\n",
    "        subgraph_density_history.append((density_score, current_subgraph))\n",
    "        density_score_history.append(density_score)\n",
    "        number_edges_history.append(current_subgraph.shape[0])\n",
    "        \n",
    "        # delete node with lowest degree\n",
    "        nodes, counts = np.unique(current_subgraph, return_counts=True)\n",
    "        idx_lowest_degree = np.argmin(counts)\n",
    "        node_lowest_degree = nodes[idx_lowest_degree]\n",
    "        \n",
    "        new_subgraph = []\n",
    "        for edge in current_subgraph:  # loop through all edges\n",
    "            if (node_lowest_degree not in edge):\n",
    "                new_subgraph.append(edge)\n",
    "            \n",
    "        \n",
    "        current_subgraph = np.array(new_subgraph)\n",
    "        number_nodes = len(np.unique(current_subgraph))\n",
    "        \n",
    "    # identify subgraph with highest density\n",
    "    densities = [density for (density, subgraph) in subgraph_density_history]\n",
    "    #print(\"densities\", densities)\n",
    "    max_ids = np.argmax(densities)\n",
    "    (max_density_score, subgraph) = subgraph_density_history[max_ids]\n",
    "    \n",
    "\n",
    "    return max_density_score, subgraph, density_score_history, number_edges_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest density score:  3.0277777777777777\n",
      "Number of edges:  109\n",
      "Number of nodes:  36\n"
     ]
    }
   ],
   "source": [
    "graph = df.to_numpy()\n",
    "\n",
    "density_score, subgraph, density_score_history, number_edges_history = greedy_dense_subgraph(graph)\n",
    "print(\"Highest density score: \", density_score)\n",
    "print(\"Number of edges: \", subgraph.shape[0])\n",
    "print(\"Number of nodes: \", len(np.unique(subgraph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(number_edges_history, density_score_history)\n",
    "plt.title(\"Progress of algorithm after every iteration\")\n",
    "plt.xlabel(\"Number of edges\")\n",
    "plt.ylabel(\"Density score\")\n",
    "plt.show()\n",
    "#plt.plot(range(len(density_score)),density_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Questions</b> \n",
    "\n",
    "* The given dolphins dataset is undirected and unweighted, and the implemented algorithm is adjusted to that\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only needed to do 4 out of 5 tasks, I decided to leave out Task 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>joke_5</th>\n",
       "      <th>joke_7</th>\n",
       "      <th>joke_8</th>\n",
       "      <th>joke_13</th>\n",
       "      <th>joke_15</th>\n",
       "      <th>joke_16</th>\n",
       "      <th>joke_17</th>\n",
       "      <th>joke_18</th>\n",
       "      <th>joke_19</th>\n",
       "      <th>joke_20</th>\n",
       "      <th>num_positives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>21844</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3403</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>23240</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  joke_5  joke_7  joke_8  joke_13  joke_15  joke_16  joke_17  \\\n",
       "0     5013       0       1       0        0        1        0        0   \n",
       "1    10016       0       0       0        1        0        1        1   \n",
       "2    21844       1       1       1        1        1        1        1   \n",
       "3     3403       1       0       0        1        0        1        1   \n",
       "4    23240       0       1       0        1        1        1        1   \n",
       "\n",
       "   joke_18  joke_19  joke_20  num_positives  \n",
       "0        1        1        1              5  \n",
       "1        1        0        1              5  \n",
       "2        1        1        1             10  \n",
       "3        0        1        1              6  \n",
       "4        1        0        1              7  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load datasets\n",
    "\n",
    "df_train = pd.read_csv(\"t4_jester-800-10.csv\")\n",
    "df_test = pd.read_csv(\"t4_test-800-10.csv\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = df_train[\"user_id\"].values\n",
    "joke_names = df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bi-partite graph\n",
    "B = nx.Graph()\n",
    "# Add nodes with the node attribute \"bipartite\"\n",
    "B.add_nodes_from(user_ids, bipartite=0)\n",
    "B.add_nodes_from(joke_names, bipartite=1)\n",
    "# Add edges only between nodes of opposite node sets\n",
    "\n",
    "set_of_edges = []\n",
    "\n",
    "for user in user_ids:\n",
    "    row = df_train.loc[df_train['user_id'] == user]\n",
    "    for joke in joke_names:\n",
    "        like = int(row[joke])\n",
    "        if (like == 1):  # add edge to the graph\n",
    "            set_of_edges.append((user, joke))\n",
    "        \n",
    "B.add_edges_from(set_of_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate simrank, everyone with everyone\n",
    "#sim = nx.simrank_similarity(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_sorted = sorted(user_ids)\n",
    "\n",
    "lol = [[sim[u][v] for v in users_sorted] for u in users_sorted]\n",
    "sim_array = np.array(lol)\n",
    "sim_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Unfortunately, I couldn't finish this task because I underestimated the computation time of the simrank algorithm. Only a few steps are missing. So when the calculating of the SimRank finishes, we get a dictionary that contains for each node the similarity score to every other node. Therefore there are dictionaries within a dictionary. We get an n x n matrix. Then we search for the test nodes. For each test node get the highest value in each row which is not the same node. So we can set the diagonal to zero to make sure this doesn't happen. With np.argmax() we the index with the node of the highest similarity score. Then we check which jokes the most similar user liked that haven't been liked so far from the test user and recommend these jokes to the test user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Questions</b> \n",
    "\n",
    "* We use collaborative filtering, because we want to give recommendations based on similar users they have watched, what they liked and therefore what could be recommended to the other person.\n",
    "* Two users are similar if they watched and liked similar movies. Hence, it is very likely that a movie liked by User 1 but not yet watched by User 2, User 2 will also very likely will enjoy.\n",
    "* Therefore, the assumption we use: The user probably likes what other similar users have liked.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movie reviews data\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "#print(documents[:1])\n",
    "#documents = np.array(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "* Remove stopwords\n",
    "* perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples']\n",
      "0.79\n",
      "Most Informative Features\n",
      "       contains(stellan) = True              pos : neg    =      8.2 : 1.0\n",
      " contains(unimaginative) = True              neg : pos    =      7.8 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      7.5 : 1.0\n",
      "     contains(atrocious) = True              neg : pos    =      6.7 : 1.0\n",
      "        contains(turkey) = True              neg : pos    =      6.7 : 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate frequency of words\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]  # first 2000 most frequent words; ordered by frequency\n",
    "print(word_features[:5])  # first five most frequent words\n",
    "\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Feature extraction and training:\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents] # c: positive or negative, d: words\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "#classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    "\n",
    "# Use of classifier:\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(5)\n",
    "\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movie reviews data\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "#print(documents[:1])\n",
    "#documents = np.array(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "* Remove stopwords\n",
    "* perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stemming:\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "porter = PorterStemmer()\n",
    "stemmed_documents = []\n",
    "for (d,c) in documents:\n",
    "    stemmed_documents.append(([porter.stem(j) for j in d], c))\n",
    "print('After stemming:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words removal and punctation removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# We include the punctation in the stop words set.\n",
    "punctation = set(\"!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~\\\"\\'\")\n",
    "stop_words.update(punctation)\n",
    "stop_words.add(\"...\")\n",
    "\n",
    "#print('Stop words that will get removed:')\n",
    "#print(stop_words)\n",
    "#print()\n",
    "\n",
    "filtered_documents = []    \n",
    "for (d,c) in stemmed_documents: \n",
    "    filtered_documents.append(([word for word in d if word not in stop_words], c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robin', 'william', 'comed', 'genu', 'one']\n",
      "0.8\n",
      "Most Informative Features\n",
      "      contains(outstand) = True              pos : neg    =     13.4 : 1.0\n",
      "          contains(plod) = True              neg : pos    =     13.1 : 1.0\n",
      "      contains(furnitur) = True              neg : pos    =      7.0 : 1.0\n",
      "       contains(sputter) = True              neg : pos    =      7.0 : 1.0\n",
      "  contains(breakthrough) = True              pos : neg    =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# calculate frequency of words\n",
    "all_words_documents = []\n",
    "for (d,c) in filtered_documents:\n",
    "    for word in d:\n",
    "        all_words_documents.append(word)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in all_words_documents)\n",
    "word_features = list(all_words)[:2000]  # first 2000 most frequent words; ordered by frequency\n",
    "print(word_features[:5])  # first five most frequent words\n",
    "\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Feature extraction and training:\n",
    "featuresets = [(document_features(d), c) for (d,c) in filtered_documents] # c: positive or negative, d: words\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "#classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    "\n",
    "# Use of classifier:\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Questions</b> \n",
    "\n",
    "Results without preprocessing:\n",
    "* Naive Bayes Classifier: 0.77\n",
    "* Support Vector Classifier: 0.77\n",
    "\n",
    "    \n",
    "Results with preprocessing (stemming and removal of stopwords):\n",
    "* Naive Bayes Classifier: 0.78\n",
    "* Support Vector Classifier: 0.8\n",
    "    \n",
    "Results and Interpretation:\n",
    "* The results get slightly better\n",
    "* Only important features were put into considertion\n",
    "* Deletion of stopwards and punctuation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace words by hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movie reviews data\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "#print(documents[:1])\n",
    "#documents = np.array(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "* Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words removal and punctation removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# We include the punctation in the stop words set.\n",
    "punctation = set(\"!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~\\\"\\'\")\n",
    "stop_words.update(punctation)\n",
    "stop_words.add(\"...\")\n",
    "\n",
    "filtered_documents = []    \n",
    "for (d,c) in documents: \n",
    "    filtered_documents.append(([word for word in d if word not in stop_words], c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words with hypernym\n",
    "hypernym_documents = []\n",
    "for (d,c) in filtered_documents:\n",
    "    hypernym_array = []\n",
    "    for word in d:\n",
    "        \n",
    "        synsets = wn.synsets(word)  # get all sysnsets\n",
    "        if (synsets == []):\n",
    "            name = word\n",
    "        else:\n",
    "            first_synset = synsets[0]  # choose first synset\n",
    "            hypernyms = first_synset.hypernyms()  # get all hypernyms of first synset\n",
    "            if (hypernyms == []):\n",
    "                name = word\n",
    "            else:\n",
    "                hypernym = hypernyms[0]  # choose first hyernym\n",
    "                lemmas = hypernym.lemmas()  # get all lemmas\n",
    "                lemma = lemmas[0]  # choose first lemma\n",
    "                \n",
    "                name = lemma.name()  # get name of lemma\n",
    "        \n",
    "        \n",
    "        hypernym_array.append(name)\n",
    "        \n",
    "    hypernym_documents.append( (hypernym_array, c) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['steve', 'martin', 'act', 'increase', 'leisure']\n",
      "0.76\n",
      "Most Informative Features\n",
      "     contains(ludicrous) = True              neg : pos    =     15.2 : 1.0\n",
      "   contains(outstanding) = True              pos : neg    =     11.1 : 1.0\n",
      "    contains(incoherent) = True              neg : pos    =      9.1 : 1.0\n",
      "        contains(feeble) = True              neg : pos    =      7.7 : 1.0\n",
      "      contains(believer) = True              pos : neg    =      6.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# calculate frequency of words\n",
    "all_words_documents = []\n",
    "for (d,c) in hypernym_documents:\n",
    "    for word in d:\n",
    "        all_words_documents.append(word)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in all_words_documents)\n",
    "word_features = list(all_words)[:2000]  # first 2000 most frequent words; ordered by frequency\n",
    "print(word_features[:5])  # first five most frequent words\n",
    "\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Feature extraction and training:\n",
    "featuresets = [(document_features(d), c) for (d,c) in hypernym_documents] # c: positive or negative, d: words\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "#classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    "\n",
    "# Use of classifier:\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Questions</b> \n",
    "\n",
    "Results:\n",
    "* Naive Bayes Classifier: 0.81\n",
    "* Support Vector Classifier: 0.8\n",
    "    \n",
    "Results and Interpretation:\n",
    "* For task b) no stemming, because wordnet synset and hypernyms work worse on stemmed words. For instance, wordnet works well on wn.synsets('happiest') or wn.synsets('happier'), but not on stemmed word of happy which is wn.synsets('happi').\n",
    "* The results improved again slightly in the case of Naive Bayes Classifier to 0.81\n",
    "* makes absolutely sense since we replace words with similar meaning to the same word\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add hypernyms of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movie reviews data\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "#print(documents[:1])\n",
    "#documents = np.array(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "* Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words removal and punctation removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# We include the punctation in the stop words set.\n",
    "punctation = set(\"!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~\\\"\\'\")\n",
    "stop_words.update(punctation)\n",
    "stop_words.add(\"...\")\n",
    "\n",
    "filtered_documents = []    \n",
    "for (d,c) in documents: \n",
    "    filtered_documents.append(([word for word in d if word not in stop_words], c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words with hypernym\n",
    "hypernym_documents = []\n",
    "for (d,c) in filtered_documents:\n",
    "    hypernym_array = []\n",
    "    for word in d:\n",
    "        hypernym_array.append(word)\n",
    "        \n",
    "        synsets = wn.synsets(word)  # get all sysnsets\n",
    "        if (synsets == []):\n",
    "            continue\n",
    "        else:\n",
    "            first_synset = synsets[0]  # choose first synset\n",
    "            hypernyms = first_synset.hypernyms()  # get all hypernyms of first synset\n",
    "            if (hypernyms == []):\n",
    "                continue\n",
    "            else:\n",
    "                hypernym = hypernyms[0]  # choose first hyernym\n",
    "                lemmas = hypernym.lemmas()  # get all lemmas\n",
    "                lemma = lemmas[0]  # choose first lemma\n",
    "                name = lemma.name()  # get name of lemma\n",
    "        \n",
    "                hypernym_array.append(name)\n",
    "        \n",
    "    hypernym_documents.append( (hypernym_array, c) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tempe', 'mills', 'cinema', 'medium', 'az']\n",
      "0.72\n",
      "Most Informative Features\n",
      "      contains(bothered) = True              neg : pos    =      9.7 : 1.0\n",
      "     contains(strongest) = True              pos : neg    =      9.0 : 1.0\n",
      "       contains(winslet) = True              pos : neg    =      7.7 : 1.0\n",
      "    contains(cronenberg) = True              pos : neg    =      7.0 : 1.0\n",
      "          contains(gump) = True              pos : neg    =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# calculate frequency of words\n",
    "all_words_documents = []\n",
    "for (d,c) in hypernym_documents:\n",
    "    for word in d:\n",
    "        all_words_documents.append(word)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in all_words_documents)\n",
    "word_features = list(all_words)[:2000]  # first 2000 most frequent words; ordered by frequency\n",
    "print(word_features[:5])  # first five most frequent words\n",
    "\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Feature extraction and training:\n",
    "featuresets = [(document_features(d), c) for (d,c) in hypernym_documents] # c: positive or negative, d: words\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "#classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    "\n",
    "# Use of classifier:\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Questions</b> \n",
    "\n",
    "Results:\n",
    "* Naive Bayes Classifier: 0.73\n",
    "* Support Vector Classifier: 0.76\n",
    "    \n",
    "Results and Interpretation:\n",
    "* For task b) no stemming, because wordnet synset and hypernyms work worse on stemmed words. For instance, wordnet works well on wn.synsets('happiest') or wn.synsets('happier'), but not on stemmed word of happy which is wn.synsets('happi').\n",
    "* The results improved got worse for both classifiers while the scores dropped to 0.73 and 0.76.\n",
    "* That makes kind of sense, because not all words have hypernyms. So imagine for instance not important words have hypernyms while important words don't have hypernyms. So we add the hypernyms of not important words. Consequently, not important words are more frequent and have more weight. That results in a worse classification score. Therefore, it is better to replace the words by its hypernym.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
